{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74f9fbd-207d-4bce-a2f1-670c4df1cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Original date diagnostics ==\n",
      "   model original_start original_end  rows  index_like_date\n",
      "0  ARIMA     1970-01-01   1970-01-01  1344             True\n",
      "1    GRU     2020-01-02   2025-03-27  1344            False\n",
      "2     RF     2020-01-03   2025-03-27  1343            False\n",
      "3     RW     2020-01-02   2025-03-27  1344            False\n",
      "4    SVR     2020-01-02   2025-03-27  1344            False\n",
      "\n",
      "Using 'GRU' as reference calendar: 2020-01-02 → 2025-03-27 (1344 unique dates)\n",
      "\n",
      "== Aligned date diagnostics ==\n",
      "   model aligned_start aligned_end  rows\n",
      "0  ARIMA    2020-01-03  2025-03-27  1343\n",
      "1    GRU    2020-01-03  2025-03-27  1343\n",
      "2     RF    2020-01-03  2025-03-27  1343\n",
      "3     RW    2020-01-03  2025-03-27  1343\n",
      "4    SVR    2020-01-03  2025-03-27  1343\n",
      "\n",
      "== Diebold–Mariano Test Statistics ==\n",
      "        ARIMA     GRU      RF      RW     SVR\n",
      "ARIMA  0.0000  7.2780  7.2650  7.2793  7.2796\n",
      "GRU   -7.2780  0.0000 -6.1098  2.8019  2.6941\n",
      "RF    -7.2650  6.1098  0.0000  6.4312  6.4683\n",
      "RW    -7.2793 -2.8019 -6.4312  0.0000  0.1853\n",
      "SVR   -7.2796 -2.6941 -6.4683 -0.1853  0.0000\n",
      "\n",
      "== Diebold–Mariano P-Values ==\n",
      "       ARIMA     GRU   RF      RW     SVR\n",
      "ARIMA    0.0  0.0000  0.0  0.0000  0.0000\n",
      "GRU      0.0  0.0000  0.0  0.0051  0.0071\n",
      "RF       0.0  0.0000  0.0  0.0000  0.0000\n",
      "RW       0.0  0.0051  0.0  0.0000  0.8530\n",
      "SVR      0.0  0.0071  0.0  0.8530  0.0000\n",
      "\n",
      "Saved files:\n",
      " - date_alignment_summary.csv\n",
      " - aligned_forecasts.csv\n",
      " - dm_statistics.csv\n",
      " - dm_p_values.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alius\\AppData\\Local\\Temp\\ipykernel_20756\\1883472139.py:32: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\alius\\AppData\\Local\\Temp\\ipykernel_20756\\1883472139.py:32: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\alius\\AppData\\Local\\Temp\\ipykernel_20756\\1883472139.py:32: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\alius\\AppData\\Local\\Temp\\ipykernel_20756\\1883472139.py:32: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\alius\\AppData\\Local\\Temp\\ipykernel_20756\\1883472139.py:32: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Align 5 forecast files to the same dates and compute a Diebold–Mariano matrix.\n",
    "\n",
    "Outputs (saved to the current directory):\n",
    "- date_alignment_summary.csv\n",
    "- aligned_forecasts.csv\n",
    "- dm_statistics.csv\n",
    "- dm_p_values.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from math import erf, sqrt\n",
    "\n",
    "# -----------------------------\n",
    "# 1) File paths + column names\n",
    "# -----------------------------\n",
    "FILES = {\n",
    "    'ARIMA': {'path': 'arima_forecast_output.csv',       'forecast_col': 'forecast',             'actual_col': 'actual'},\n",
    "    'GRU':   {'path': 'gru_forecast_output.csv',         'forecast_col': 'predicted',            'actual_col': 'actual'},\n",
    "    'RF':    {'path': 'random_forest_predictions.csv',   'forecast_col': 'prediction',           'actual_col': 'actual'},\n",
    "    'RW':    {'path': 'random_walk_forecast_output.csv', 'forecast_col': 'random_walk_forecast', 'actual_col': 'value'},\n",
    "    'SVR':   {'path': 'svr_forecast_output.csv',         'forecast_col': 'predicted',            'actual_col': 'actual'},\n",
    "}\n",
    "\n",
    "# ---------------------------------\n",
    "# 2) Helper functions for date fix\n",
    "# ---------------------------------\n",
    "def to_datetime_norm(s):\n",
    "    \"\"\"Parse dates robustly -> timezone-naive midnight (YYYY-MM-DD 00:00:00).\"\"\"\n",
    "    dt = pd.to_datetime(s, errors='coerce', infer_datetime_format=True)\n",
    "    try:\n",
    "        dt = dt.dt.tz_localize(None)\n",
    "    except Exception:\n",
    "        try:\n",
    "            dt = dt.dt.tz_convert(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return dt.dt.normalize()\n",
    "\n",
    "def is_index_like_date_column(raw_series, parsed_dt):\n",
    "    \"\"\"\n",
    "    Heuristics to flag 'date' that are actually indices (e.g., integers) or invalid:\n",
    "    - Original dtype numeric OR\n",
    "    - Parsed years all <= 1971 (near epoch) OR\n",
    "    - Parsed all the same day OR\n",
    "    - Mostly NaT after parsing\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_numeric_dtype(raw_series):\n",
    "        return True\n",
    "    if parsed_dt.isna().mean() > 0.5:  # >50% failed parse\n",
    "        return True\n",
    "    years = parsed_dt.dropna().dt.year\n",
    "    if len(years) == 0:\n",
    "        return True\n",
    "    if years.min() <= 1971 and years.max() <= 1971:\n",
    "        return True\n",
    "    if parsed_dt.nunique(dropna=True) == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normal_sf(z):\n",
    "    \"\"\"Survival function 1 - Phi(|z|) using error function, no SciPy needed.\"\"\"\n",
    "    return 0.5 * (1 - erf(abs(z) / sqrt(2)))\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3) Load files, standardize, inspect dates\n",
    "# -----------------------------------------\n",
    "raw = {}\n",
    "before_rows = []\n",
    "for model, info in FILES.items():\n",
    "    df = pd.read_csv(info['path'])\n",
    "    # standardize columns\n",
    "    df = df.rename(columns={info['forecast_col']: 'forecast', info['actual_col']: 'actual'})\n",
    "    if 'date' not in df.columns:\n",
    "        raise ValueError(f\"{model}: missing 'date' column.\")\n",
    "    df['date_raw'] = df['date']  # keep for diagnostics\n",
    "    dt = to_datetime_norm(df['date'])\n",
    "    idx_like = is_index_like_date_column(df['date'], dt)\n",
    "\n",
    "    raw[model] = {\n",
    "        'df': df[['date', 'date_raw', 'actual', 'forecast']].copy(),\n",
    "        'dt': dt,\n",
    "        'index_like': idx_like\n",
    "    }\n",
    "    before_rows.append({\n",
    "        'model': model,\n",
    "        'original_start': dt.min(),\n",
    "        'original_end': dt.max(),\n",
    "        'rows': len(df),\n",
    "        'index_like_date': idx_like\n",
    "    })\n",
    "\n",
    "before_df = pd.DataFrame(before_rows).sort_values('model').reset_index(drop=True)\n",
    "print(\"== Original date diagnostics ==\")\n",
    "print(before_df)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) Choose a reference calendar among valid-dated dataframes\n",
    "# -----------------------------------------------------------\n",
    "candidates = [(m, info['dt'], info['index_like'], len(info['df'])) for m, info in raw.items()]\n",
    "valid = [(m, dt, n) for (m, dt, ix, n) in candidates if not ix and dt.notna().any()]\n",
    "if not valid:\n",
    "    raise ValueError(\"No file with valid real dates to use as a reference calendar.\")\n",
    "\n",
    "# Pick the valid model with the most rows as reference\n",
    "ref_model, ref_dt, ref_n = sorted(valid, key=lambda x: (-x[2], x[1].min()))[0]\n",
    "ref_calendar = pd.Series(ref_dt.dropna().sort_values().unique())\n",
    "print(f\"\\nUsing '{ref_model}' as reference calendar: {ref_calendar.iloc[0].date()} → {ref_calendar.iloc[-1].date()} \"\n",
    "      f\"({len(ref_calendar)} unique dates)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5) Build fixed dataframes with proper 'date' column set\n",
    "# ---------------------------------------------------------\n",
    "fixed = {}\n",
    "issues = []\n",
    "for m, info in raw.items():\n",
    "    df = info['df'].copy()\n",
    "    if info['index_like']:\n",
    "        # Try to borrow the reference calendar by order if lengths match\n",
    "        if len(df) == len(ref_calendar):\n",
    "            df['date'] = ref_calendar.values\n",
    "        else:\n",
    "            issues.append((m, len(df), len(ref_calendar)))\n",
    "            # Fallback: keep parsed (likely invalid) dates to fail later gracefully\n",
    "            df['date'] = to_datetime_norm(df['date'])\n",
    "    else:\n",
    "        df['date'] = info['dt']  # already parsed valid dates\n",
    "\n",
    "    # Keep only necessary columns and sort\n",
    "    df = df[['date', 'actual', 'forecast']].dropna(subset=['date']).sort_values('date').reset_index(drop=True)\n",
    "    fixed[m] = df\n",
    "\n",
    "if issues:\n",
    "    raise ValueError(\n",
    "        \"Cannot fix index-like dates for these files because their lengths don't match the reference calendar:\\n\"\n",
    "        + \"\\n\".join([f\" - {m}: len={n} vs ref={r}\" for (m, n, r) in issues])\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) Align all to the exact intersection of available dates\n",
    "# -------------------------------------------------------\n",
    "date_sets = [set(df['date']) for df in fixed.values()]\n",
    "common_dates = sorted(set.intersection(*date_sets))\n",
    "if not common_dates:\n",
    "    raise ValueError(\"No overlapping dates across all models after fixing dates.\")\n",
    "\n",
    "# Trim each DF to the common calendar\n",
    "for m in fixed:\n",
    "    fixed[m] = fixed[m][fixed[m]['date'].isin(common_dates)].sort_values('date').reset_index(drop=True)\n",
    "\n",
    "after_rows = []\n",
    "for m, df in fixed.items():\n",
    "    after_rows.append({\n",
    "        'model': m, 'aligned_start': df['date'].min(),\n",
    "        'aligned_end': df['date'].max(), 'rows': len(df)\n",
    "    })\n",
    "after_df = pd.DataFrame(after_rows).sort_values('model').reset_index(drop=True)\n",
    "print(\"\\n== Aligned date diagnostics ==\")\n",
    "print(after_df)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7) Build combined panel for actual & forecasts\n",
    "# ---------------------------------------------\n",
    "combined = pd.DataFrame({'date': pd.Series(common_dates)})\n",
    "# Merge each model's actual & forecast; keep per-model actuals for robust combining\n",
    "for m, df in fixed.items():\n",
    "    tmp = df[['date', 'actual', 'forecast']].copy()\n",
    "    tmp = tmp.rename(columns={'actual': f'actual_{m}', 'forecast': m})\n",
    "    combined = combined.merge(tmp, on='date', how='inner')\n",
    "\n",
    "# If all 'actual_*' equal, just take one; else take row-wise median (robust)\n",
    "actual_cols = [c for c in combined.columns if c.startswith('actual_')]\n",
    "if len(actual_cols) == 0:\n",
    "    raise ValueError(\"No 'actual' columns found after merge.\")\n",
    "equal_actuals = all(combined[actual_cols[0]].equals(combined[c]) for c in actual_cols[1:])\n",
    "if equal_actuals:\n",
    "    combined['actual'] = combined[actual_cols[0]].values\n",
    "else:\n",
    "    combined['actual'] = np.nanmedian(combined[actual_cols].values, axis=1)\n",
    "\n",
    "# Optional: drop per-model actuals to keep the table tidy\n",
    "# combined = combined.drop(columns=actual_cols)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8) Diebold–Mariano test (squared-error loss) + HAC (NW)\n",
    "# ---------------------------------------------------------\n",
    "def dm_test(e1, e2, hac_lags=None):\n",
    "    \"\"\"\n",
    "    e1, e2: np.array forecast errors (actual - forecast), same length\n",
    "    hac_lags: Newey–West bandwidth; default ~ 1.5*T^(1/3)\n",
    "    Returns: (DM_stat, two-sided p-value)\n",
    "    \"\"\"\n",
    "    e1 = np.asarray(e1, float)\n",
    "    e2 = np.asarray(e2, float)\n",
    "    if len(e1) != len(e2):\n",
    "        raise ValueError(\"Errors must have same length.\")\n",
    "\n",
    "    d = (e1**2 - e2**2)\n",
    "    T = len(d)\n",
    "    d_bar = d.mean()\n",
    "\n",
    "    if hac_lags is None:\n",
    "        hac_lags = int(np.floor(1.5 * (T ** (1/3))))  # automatic bandwidth\n",
    "\n",
    "    # Newey–West HAC variance estimate of d\n",
    "    gamma0 = np.var(d, ddof=1)\n",
    "    var_hat = gamma0\n",
    "    for k in range(1, min(hac_lags, T-1) + 1):\n",
    "        w = 1.0 - k / (hac_lags + 1.0)  # Bartlett weight\n",
    "        cov = np.cov(d[k:], d[:-k], bias=True)[0, 1]\n",
    "        var_hat += 2.0 * w * cov\n",
    "\n",
    "    var_dbar = var_hat / T if T > 0 else np.nan\n",
    "    if not np.isfinite(var_dbar) or var_dbar <= 0:\n",
    "        return 0.0, 1.0  # fallback\n",
    "\n",
    "    DM = d_bar / np.sqrt(var_dbar)\n",
    "    # two-sided p; Phi via erf\n",
    "    pval = 2 * normal_sf(DM)\n",
    "    return DM, pval\n",
    "\n",
    "models = list(fixed.keys())\n",
    "dm_stats = pd.DataFrame(0.0, index=models, columns=models)\n",
    "dm_pvals = pd.DataFrame(0.0, index=models, columns=models)\n",
    "\n",
    "errors = {m: (combined['actual'].values - combined[m].values).astype(float) for m in models}\n",
    "for m1, m2 in combinations(models, 2):\n",
    "    stat, pval = dm_test(errors[m1], errors[m2])\n",
    "    dm_stats.loc[m1, m2] = stat\n",
    "    dm_stats.loc[m2, m1] = -stat  # antisymmetry of d(e1^2 - e2^2)\n",
    "    dm_pvals.loc[m1, m2] = pval\n",
    "    dm_pvals.loc[m2, m1] = pval   # symmetric p-values\n",
    "\n",
    "# Diagonals\n",
    "np.fill_diagonal(dm_stats.values, 0.0)\n",
    "np.fill_diagonal(dm_pvals.values, 0.0)\n",
    "\n",
    "# -----------------------\n",
    "# 9) Save + print result\n",
    "# -----------------------\n",
    "before_df.to_csv('date_alignment_summary.csv', index=False)\n",
    "combined.to_csv('aligned_forecasts.csv', index=False)\n",
    "dm_stats.to_csv('dm_statistics.csv')\n",
    "dm_pvals.to_csv('dm_p_values.csv')\n",
    "\n",
    "print(\"\\n== Diebold–Mariano Test Statistics ==\")\n",
    "print(dm_stats.round(4))\n",
    "print(\"\\n== Diebold–Mariano P-Values ==\")\n",
    "print(dm_pvals.round(4))\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\" - date_alignment_summary.csv\")\n",
    "print(\" - aligned_forecasts.csv\")\n",
    "print(\" - dm_statistics.csv\")\n",
    "print(\" - dm_p_values.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df368ae-ef48-4286-bf37-0b0907919982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
